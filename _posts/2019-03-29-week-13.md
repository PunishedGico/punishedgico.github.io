---
layout: post
title: Week 13
subtitle: Training my own model, sort of
---

It was time to train my own model for detecting ladders, bollard and ropes. Unfortunately this means labeling data. Lorenz already used an online tool called [Labelbox](www.labelbox.com) for labeling their data.

![labelbox](/img/labelbox.jpg)

I used my Framepicker to extract around 350 images from various videos from harbours recorded by Lorenz' drones. These images would serve as the basis for my dataset.

The Object Detection API uses a specific format for it's datasets, the TFRecord. Luckily Labelbox is able to export to that directly which made getting started a lot easier.

It was now time to choose a [model](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md#coco-trained-models). Training an object detector from scratch would take hundreds of thousands of images, therefore we use something called transfer learning. It involves retraining the top layer of an already trained model. So we can take advantage of a general purpose object detector, trained on a massive dataset, to simplify the process for us. This makes it possible to achieve good detection rates, with a much smaller dataset.

I decided to go with the SSD Mobilenet v2 model, since its good balance of speed and accuracy would allow it to run directly on the drone if it was needed.

TensorFlow comes with a neat thing called TensorBoard, which allows you to keep track of how your training is going.

![tensorboard](https://www.tensorflow.org/images/mnist_tensorboard.png)

After training was done, it was time to test it on our footage.

![ssd](/img/ssd.jpg)
